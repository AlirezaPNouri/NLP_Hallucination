In this repository, there are experiemnts to detect hallucination on summaries generated by an LLM 
